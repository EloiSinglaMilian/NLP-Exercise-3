###########################################
# IMPORTING LIBRARIES AND SETTING VARIABLES
###########################################


import os
import re
import spacy
from spacy.training import Example
from spacy.util import (
    minibatch,
    compile_infix_regex,
    compile_suffix_regex,
    compile_prefix_regex,
    fix_random_seed,
)
from spacy.symbols import ORTH
from spacy.scorer import Scorer
import random
import numpy as np


# We set a seed for the random and spaCy modules
# And also for numpy just in case some other module has it as a dependency
# Still results seem to not be reproducible
# This may be due to thread assignment in the GPU
# It seems there have been problems like this in the past
# However, we prefer being able to use multiple threads than having determinism
seed = 110997
random.seed(seed)
fix_random_seed(seed)
np.random.seed(seed)


# This is the adress to where the corpus is
# It should end in a forward slash
folder_path = "/corpus_ex3/"


# Number of epochs to train for
# And batch size
n_epochs = 10
batch_size = 8

# Percentage (from 0 to 1) of parameters to drop each epoch
# To avoid overfitting (does not seem to improve results here)
drop_param = 0.0


# Set to true if you want to see the accuracy on the test set
# After training
print_test_acc = True


# This string is for a new text to check tokenization and to tag after training
text_to_tag = (
    "bha mi a' leughadh mu gheamannan a bhathar a' cluich ann an uibhist còrr is ceud "
    "bliadhna air ais. 's ann anns an iris the celtic review a tha an cunntas. chaidh a "
    "sgrìobhadh le fear alasdair moireasdan. tha tuairisgeul ann de dh'fhichead geama. tha mi "
    "airson innse dhuibh mu chuid dhiubh."
)


##############################
# READING AND PROCESSING FILES
##############################


# We initialize two empty lists to store the strings
# 'files' is for documents with proper punctuation
# 'files_no_punct' is for the rest
files = []
files_no_punct = []

# We iterate over every .txt document in the folder
# Check to which group it belongs
# (With punctuation or without, we just know which are which)
# And append it to the correct list
for filename in os.listdir(folder_path):
    if filename.endswith(".txt"):
        with open(os.path.join(folder_path, filename), "r", encoding="utf-8") as file:
            if (
                filename.startswith("f")
                or filename.startswith("n")
                or filename.startswith("pw")
            ):
                files.append(file.read())
            else:
                files_no_punct.append(file.read())


# This function takes a string text like our corpus
# That is formatted by word/tag pairs
# And converts them into (word, tag) tuples
# The main challenge was multiword tokens like "ann an/Sp"
# It returns a list of tuples
def extract_word_tag_pairs(text):
    pattern = re.compile(r"(?P<word>.+?)/(?P<tag>[a-zA-Z0-9\-\'\*\+\.\)\]]+)(?=\s|$)")
    return [
        (m.group("word").strip().lower(), m.group("tag").strip())
        for m in pattern.finditer(text)
    ]


# This function takes the list of strings that we generated by reading the files
# Applies the extract_word_tag_pairs function to get a list of list of (word, tag) tuples
# Then separates the coordinating conjunctions and subordinating ones
# By giving a new letter to the latter
# And finally converts the tag to only the first letter
# It also generates a set of all word and tags
def process_files(files):

    # First we replace the different types of apostrophes and quotation marks
    # Into normal ones and we also delete a weird symbol that appears sometimes
    replace_map = str.maketrans({"‘": "'", "’": "'", "“": '"', "”": '"', "\ufeff": ""})
    files = [text.translate(replace_map) for text in files]

    # We then apply the function we defined above to separate the word-tag pairs
    processed_files = [extract_word_tag_pairs(text) for text in files]

    # We create a set of all words and another one of all tags
    # For debugging purposes
    word_set = set(word for file in processed_files for (word, tag) in file)
    tag_set = set(tag for file in processed_files for (word, tag) in file)

    # We give subordinating conjunctions their own tag
    processed_files = [
        [
            (word, "Z") if tag == "Cs" or tag == "Csw" else (word, tag)
            for (word, tag) in file
        ]
        for file in processed_files
    ]

    # We get take only the first letter of the tag and convert it to uppercase
    processed_files = [
        [(word, tag[0].upper() if tag else "") for (word, tag) in file]
        for file in processed_files
    ]

    # We get a set of all final tags (only uppercase letters) for debugging
    final_tag_set = set(tag for file in processed_files for (word, tag) in file)

    # We return the processed files and the three sets
    return processed_files, word_set, tag_set, final_tag_set


# We clean both the punctuated files and the punctuationless
files, word_set, tag_set, final_tag_set = process_files(files)
files_no_punct, word_set_no_punct, tag_set_no_punct, final_tag_set_no_punct = (
    process_files(files_no_punct)
)


# We check the symbols that occur in the cleaned files to find weird stuff
# And refine the regex or normalize things like the apostrophes
character_set = set(c for file in files for word, tag in file for c in word)
print("All characters in words:", sorted(character_set))

# We check that all tags are present in the data
print(
    "All tags in the final tag set (before conversion to spaCy tags):",
    sorted(set(tag for tag in final_tag_set)),
)


# We combine both sets of of words to get a single one
full_word_set = sorted(set(list(word_set) + list(word_set_no_punct)))

# And then make a list out of all the ones that have a space inside
# This will be useful to make a list for the tokenizer
# To learn to not separate multiwork tokens
full_multiword_token_list = [w for w in list(full_word_set) if " " in w]


########################
# INITIALIZING THE MODEL
########################


# We create a blank multilingual model
nlp = spacy.blank("xx")


# We are now going to tweak the tokenizer
# We want it to not separate words that contain apostrophes or dashes
# We get the list of infixes that the default tokenizer looks for
infixes = nlp.Defaults.infixes

# Then we remove every pattern that has either an apostrophe or a dash
new_infixes = [
    pattern
    for pattern in infixes
    if "'" not in pattern and "’" not in pattern and "-" not in pattern
]

# We compile the new regex without those rules
infix_re = compile_infix_regex(new_infixes)

# And update the tokenizer with it
nlp.tokenizer.infix_finditer = infix_re.finditer

# We do the same with suffixes
suffixes = nlp.Defaults.suffixes
new_suffixes = [
    pattern
    for pattern in suffixes
    if "'" not in pattern and "’" not in pattern and "-" not in pattern
]
suffix_re = compile_suffix_regex(new_suffixes)
nlp.tokenizer.suffix_search = suffix_re.search

# And also with prefixes
prefixes = nlp.Defaults.prefixes
new_prefixes = [
    pattern
    for pattern in prefixes
    if "'" not in pattern and "’" not in pattern and "-" not in pattern
]
prefix_re = compile_prefix_regex(new_prefixes)
nlp.tokenizer.prefix_search = prefix_re.search


# We also give the tokenizer all the multiword tokens we had
# So that it knows not to break them apart
for token in full_multiword_token_list:
    nlp.tokenizer.add_special_case(token, [{ORTH: token}])

# We check how the model tokenizes our testing text
doc = nlp(text_to_tag)
print("Tokenization of test text:", [token.text for token in doc])


##########################################
# CREATING TRAINING DATA AND ADDING TAGGER
##########################################


# We go over the lists of tuples and check for periods or quotes
# And we split the sentences there
# To create a list of lists of tuples
# But this time the inner lists are not files, but rather only sentences
sentences = []
for file in files:
    sentence = []
    for word, tag in file:
        sentence.append((word, tag))
        if word == "." or word == '"':
            sentences.append(sentence)
            sentence = []
    if sentence:
        sentences.append(sentence)


# This dictionary maps our set of annotation
# To the ones that are more common in spaCy
tag_map = {
    "N": {"POS": "NOUN"},
    "V": {"POS": "VERB"},
    "A": {"POS": "ADJ"},
    "P": {"POS": "PRON"},
    "D": {"POS": "DET"},
    "T": {"POS": "DET"},
    "R": {"POS": "ADV"},
    "S": {"POS": "ADP"},
    "C": {"POS": "CCON"},  # We separated the original C into C (Cc) and Z (Cs)
    "Z": {"POS": "SCON"},
    "M": {"POS": "NUM"},
    "I": {"POS": "INTJ"},
    "U": {"POS": "X"},
    "X": {"POS": "SYM"},
    "F": {"POS": "PUNCT"},
    "Y": {"POS": "SYM"},  # We merged abbreviations and residuals
    "W": {"POS": "AUX"},
    "Q": {"POS": "PART"},
}


# We add a tagger to our model
if "tagger" not in nlp.pipe_names:
    tagger = nlp.add_pipe("tagger")


# We create a list to append our training examples to
train_data = []

# We iterate over each sentence in the processed corpus
for sent in sentences:

    # We create a list of (word, tag) tuples by converting the older ones
    # To use the correct tags for spaCy that are specifiec in the dictionary above
    sentence = [(word, tag_map.get(tag, {}).get("POS", "X")) for word, tag in sent]

    # We eliminate any possible empty strings
    sentence = [(word, tag) for word, tag in sentence if word.strip()]

    # We skip any possible empty sentence
    if not sentence:
        continue

    # We process the sentence with our model
    doc = spacy.tokens.Doc(nlp.vocab, words=[word for word, _ in sentence])

    # We manually add the tags to the doc
    for i, (_, tag) in enumerate(sentence):
        doc[i].tag_ = tag

    # We create a spaCy example and append it
    example = Example.from_dict(doc, {"tags": [tag for _, tag in sentence]})
    train_data.append(example)


# We create a set to add our tags to
all_tags = set()

# We iterate over every example
for example in train_data:

    # Inside the example we iterate over every tokken
    for token in example.reference:

        # And add its tag to the set
        all_tags.add(token.tag_)

# We add these tags to the tagger
for tag in all_tags:
    tagger.add_label(tag)


# We split the data into training and validation (8:1:1 split)
train_data, dev_data = (
    train_data[: int(len(train_data) * 0.8)],
    train_data[int(len(train_data) * 0.8) :],
)
dev_data, test_data = (
    dev_data[: int(len(dev_data) * 0.5)],
    dev_data[int(len(dev_data) * 0.5) :],
)

# We print the length of each set of data
print("Length of train data:\t", len(train_data))
print("Length of dev data:\t", len(dev_data))
print("Length of test data:\t", len(test_data))


#######################
# CHECKING TOKENIZATION
#######################


# We select which data we want to check tokenization on
tokenization_check_data = train_data


# We initialize a counter for sentences in which the lenght of our tokenization
# And of the golden one don't match
n_wrong_tokenizations = 0

# We iterate over every example in the data set
# Get the golden tokens and the predicted tokens
# And if their lengths don't match
# We add one to the counter
for i, example in enumerate(tokenization_check_data):
    doc = nlp(example.text)
    training_tokens = [token.text for token in example.reference]
    predicted_tokens = [token.text for token in doc]

    if len(training_tokens) != len(predicted_tokens):
        n_wrong_tokenizations += 1

# We print the percentage of examples in which the lengths don't match
print(
    f"Percentage of tokenization length mismatches in data set: {n_wrong_tokenizations/len(tokenization_check_data)*100:.2f}"
)


####################
# MAIN TRAINING LOOP
####################


# We start the training by setting an optimizer
optimizer = nlp.begin_training()


# This function evaluates the performance of a model on a list of examples
# It returns a spaCy score
def evaluate(nlp, examples):

    # We define a scorer object
    scorer = Scorer()

    # We initialize an empty list for processed examples
    processed_example_list = []

    # We iterate over every example in the list
    for gold_example in examples:

        # We process the text of the example with our model
        pred_doc = nlp(gold_example.text)

        # We align what we predicted with the golden labels
        # To account for wrong tokenizations
        example = Example(pred_doc, gold_example.reference)
        processed_example_list.append(example)

    # We calculate the score and return it
    scores = scorer.score(processed_example_list)
    return scores


# We train the model for as many epochs as we defined
for epoch in range(n_epochs):

    # We first shuffle the data and initialize a dictionary for losses
    # Although we just have a tagger
    random.shuffle(train_data)
    losses = {}

    # We get a list of batches using a spaCy funtion and iterate over them
    batches = minibatch(train_data, size=batch_size)
    for batch in batches:

        # We initialize an empty set to store processed examples
        # And iterate over each in the batch
        examples = []
        for example_data in batch:

            # We process the example text with our model
            pred_doc = nlp.make_doc(example_data.text)

            # We align it with the golden labels
            # To account for incorrect tokenization
            # And append it to the list
            example = Example(pred_doc, example_data.reference)
            examples.append(example)

        # We update our nlp based on the losses in the example list
        nlp.update(examples, sgd=optimizer, losses=losses, drop=drop_param)

    # We print the loss
    print(f"Epoch {epoch+1} - Loss: {losses}")

    # And evaluate the accuracy on train and dev data
    train_scores = evaluate(nlp, train_data)
    dev_scores = evaluate(nlp, dev_data)

    # And print them as percentages
    print(f"Epoch {epoch+1} - Train Tag Accuracy: {train_scores['tag_acc'] * 100:.2f}%")
    print(f"Epoch {epoch+1} - Dev Tag Accuracy: {dev_scores['tag_acc'] * 100:.2f}%\n")


# If set to True, we check the accuracy on the test data
if print_test_acc:
    test_scores = evaluate(nlp, test_data)
    print(f"Train Tag Accuracy: {test_scores['tag_acc'] * 100:.2f}%")


##################
# TAGGING NEW TEXT
#################


# We process the new text
doc = nlp(text_to_tag)

# And we print the words and their predicted POS tags to check manually
for token in doc:
    print(f"{token.text}: {token.tag_}")
